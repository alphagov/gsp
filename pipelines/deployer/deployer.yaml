groups:
- name: deploy
  jobs:
  - update
  - deploy
  - apply
  - check-canary
  - check-logging
  - check-conformance
  - check-tools
- name: destroy
  jobs:
  - destroy

unpack_release: &unpack_release
  platform: linux
  params:
    CLUSTER_PUBLIC_KEY:
  run:
    path: /bin/bash
    args:
    - -euo
    - pipefail
    - -c
    - |
      echo "preparing keyring to verify release..."
      echo "${CLUSTER_PUBLIC_KEY}" > key
      gpg --import key
      gpg --verify gsp/source.tar.gz.asc
      echo "unpacking src tarball..."
      tar -xvf gsp/source.tar.gz -C platform --strip-components=1
  inputs:
  - name: gsp
  outputs:
  - name: platform

generate_cluster_values: &generate_cluster_values
  platform: linux
  run:
    path: /bin/bash
    args:
    - -euo
    - pipefail
    - -c
    - |
      mkdir -p cluster-values
      echo "fetching cluster-values file from cluster-state..."
      jq -r '.values' ./cluster-state/metadata > ./cluster-values/values.yaml
      echo "OK!"
  inputs:
  - name: cluster-state
  outputs:
  - name: cluster-values

generate_user_values: &generate_user_values
  platform: linux
  params:
    ACCOUNT_ID:
    ACCOUNT_NAME:
    CLUSTER_NAME:
  run:
    path: /bin/bash
    args:
    - -euo
    - pipefail
    - -c
    - |
      cd users
      echo "creating helm compatible values file from user data"
      yq '.[]' "${ACCOUNT_NAME}-trusted-developers.yaml" \
        | jq ". + {roleARN: (\"arn:aws:iam::${ACCOUNT_ID}:role/${CLUSTER_NAME}-user-\" + .name)}" \
        | jq -s '{users: .}' \
        | yq --yaml-output .\
        > ../user-values/values.yaml
      cat ../user-values/values.yaml
  inputs:
  - name: users
  outputs:
  - name: user-values

generate_kube_values: &generate_kube_values
  platform: linux
  params:
    ACCOUNT_ROLE_ARN:
    CLUSTER_NAME:
    AWS_REGION:
    AWS_DEFAULT_REGION:
  run:
    path: /bin/bash
    args:
    - -eu
    - -c
    - |
      mkdir -p kube-values
      echo "assuming aws deployer role..."
      AWS_CREDS="$(aws-assume-role $ACCOUNT_ROLE_ARN)"
      eval "${AWS_CREDS}"
      echo "fetching kubeconfig from aws..."
      aws eks update-kubeconfig \
        --name "${CLUSTER_NAME}" \
        --kubeconfig ./kubeconfig
      export KUBECONFIG=$(pwd)/kubeconfig
      kubectl -n default get endpoints kubernetes -o json \
        | jq '{"global": {"kubeApiService": {"endpointCidrs": [.subsets[].addresses[].ip + "/32"]}}}' | yq --yaml-output . \
        > kube-values/values.yaml
  outputs:
  - name: kube-values

generate_users_terraform: &generate_users_terraform
  platform: linux
  params:
    ACCOUNT_NAME:
  run:
    path: /bin/bash
    args:
    - -euo
    - pipefail
    - -c
    - |
      mkdir -p users-terraform
      cd users
      echo "creating terraform for user roles from user data"
      yq '.[]' "${ACCOUNT_NAME}-trusted-developers.yaml" \
        | jq '[{key: (.name | gsub("[^a-z-A-Z0-9]"; "-")), value: {source: "../platform/modules/gsp-user", user_name: .name, user_arn: .ARN, cluster_name: "${var.cluster_name}"}}] | from_entries' \
        | jq -s '{module: . | add, variable: { aws_account_role_arn: { type: "string" }, cluster_name: { type: "string" } }, provider: { aws: { region: "eu-west-2", assume_role: { role_arn: "${var.aws_account_role_arn}" } } } }' \
        > ../users-terraform/users.tf.json
      cat ../users-terraform/users.tf.json
  inputs:
  - name: users
  outputs:
  - name: users-terraform

apply_cluster_chart: &apply_cluster_chart
  platform: linux
  params:
    ACCOUNT_ROLE_ARN:
    ACCOUNT_NAME:
    CLUSTER_NAME:
    AWS_REGION:
    AWS_DEFAULT_REGION:
    CHART_NAME:
    DEFAULT_NAMESPACE:
    CHART_RELEASE_NAME:
    GITHUB_API_TOKEN:
    CLUSTER_PRIVATE_KEY:
    CLUSTER_PUBLIC_KEY:
    CONFIG_VALUES_PATH:
  run:
    path: /bin/bash
    args:
    - -eu
    - -c
    - |
      echo "preparing keyring..."
      echo "${CLUSTER_PUBLIC_KEY}" > key
      gpg --import key
      gpg --export > ~/.gnupg/pubring.gpg
      echo "verifying gsp-cluster chart signature..."
      helm verify gsp/gsp-cluster-*.tgz && echo "OK!"
      echo "verifying gsp-cluster chart signature..."
      helm verify gsp/gsp-istio-*.tgz && echo "OK!"
      echo "assuming aws deployer role..."
      AWS_CREDS="$(aws-assume-role $ACCOUNT_ROLE_ARN)"
      eval "${AWS_CREDS}"
      echo "fetching kubeconfig from aws..."
      aws eks update-kubeconfig \
        --name "${CLUSTER_NAME}" \
        --kubeconfig ./kubeconfig
      export KUBECONFIG=$(pwd)/kubeconfig
      echo "setting default namespace to ${DEFAULT_NAMESPACE}"
      kubectl config set-context $(kubectl config get-contexts -o name) \
        --namespace "${DEFAULT_NAMESPACE}"
      echo "fetching release tag"
      RELEASE_TAG=$(cat gsp/tag)
      echo "RELEASE_TAG=${RELEASE_TAG}"
      echo "rendering ${CHART_NAME} chart..."
      mkdir -p manifests
      helm template \
        --name "${CHART_RELEASE_NAME}" \
        --namespace "${DEFAULT_NAMESPACE}" \
        --values cluster-values/values.yaml \
        --values user-values/values.yaml \
        --values kube-values/values.yaml \
        --values config/${CONFIG_VALUES_PATH} \
        --set "githubAPIToken=${GITHUB_API_TOKEN}" \
        --set "global.cluster.privateKey=${CLUSTER_PRIVATE_KEY}" \
        --set "global.cluster.publicKey=${CLUSTER_PUBLIC_KEY}" \
        --output-dir manifests \
        gsp/gsp-cluster-*.tgz
      echo "rendering gsp-istio chart..."
      helm template \
        --name istio \
        --namespace istio-system \
        --output-dir manifests \
        --values config/${CONFIG_VALUES_PATH} \
        --set global.runningOnAws=true \
        gsp/gsp-istio-*.tgz
      function apply() {
        echo "applying ${1} from ${CHART_NAME} chart..."
        until kubectl apply -R -f $1; do
          echo "---> ${1} apply failed retrying in 5s..."
          sleep 5
        done
        sleep 5 # FIXME: we should do something smarter than sleep and check for success
        echo "---> ${1} applied OK!"
      }
      apply manifests/${CHART_NAME}/templates/00-aws-auth/
      apply manifests/gsp-istio
      apply manifests/${CHART_NAME}/templates/01-aws-system/
      apply manifests/${CHART_NAME}/templates/02-gsp-system/
      apply manifests/${CHART_NAME}/templates/03-namespaces/
      apply manifests/
  inputs:
  - name: cluster-values
  - name: config
  - name: user-values
  - name: gsp
  - name: kube-values

check_conformance: &check_conformance
  platform: linux
  params:
    ACCOUNT_ROLE_ARN:
    ACCOUNT_NAME:
    CLUSTER_NAME:
    DEFAULT_NAMESPACE:
    AWS_REGION:
    AWS_DEFAULT_REGION:
  run:
    path: /bin/bash
    args:
    - -eu
    - -c
    - |
      echo "assuming aws deployer role..."
      AWS_CREDS="$(aws-assume-role $ACCOUNT_ROLE_ARN)"
      eval "${AWS_CREDS}"
      echo "fetching kubeconfig from aws..."
      aws eks update-kubeconfig \
        --name "${CLUSTER_NAME}" \
        --kubeconfig ./kubeconfig
      export KUBECONFIG=$(pwd)/kubeconfig
      echo "setting default namespace to ${DEFAULT_NAMESPACE}"
      kubectl config set-context $(kubectl config get-contexts -o name) \
        --namespace "${DEFAULT_NAMESPACE}"

      echo "beginning conformance test..."
      mkdir -p plugins/e2e/results

      function cleanup() {
        echo "cleaning up sonobuoy..."
         sonobuoy delete --wait
      }
      trap 'cleanup' INT TERM EXIT

      sonobuoy run \
        --wait \
        --sonobuoy-image "gcr.io/heptio-images/sonobuoy:v0.14.3" \
        --plugin e2e \
        --e2e-focus "Pods should be submitted and removed" \
        --kube-conformance-image "govsvc/conformance-amd64:0.0.1559644071" \
        --plugin-env e2e.ALLOWED_NOT_READY_NODES=$(kubectl get nodes --selector "! node-role.kubernetes.io/worker"  --no-headers=true | wc -l) # only wait for worker nodes

      sleep 10 # wait for results to be written
      results=$(sonobuoy retrieve)
      sonobuoy e2e ${results}
      passed=$(sonobuoy e2e ${results} --show passed | head -n1)
      failed=$(sonobuoy e2e ${results} --show failed | head -n1)

      if [[ ${passed} == "passed tests: 1" && ${failed} == "failed tests: 0" ]]; then
        echo "SUCCESS"
        exit 0
      fi

      echo "FAIL"
      exit 1


check_canary: &check_canary
  platform: linux
  params:
    CLUSTER_DOMAIN:
  run:
    path: /bin/bash
    args:
      - -eu
      - -c
      - |
        now="$(date '+%s')"
        echo "Current time: ${now}"
        echo "pinging https://canary.${CLUSTER_DOMAIN}/metrics to check ingress, expecting the deployment to happen soon..."
        while true; do
          last_deploy="$(curl --silent --show-error --max-time 5 --fail https://canary.${CLUSTER_DOMAIN}/metrics | awk '$1 ~ /^canary_build_timestamp/ {print $2 * 2 / 2}')"
          if [ "${last_deploy}" -ge "${now}" ]; then
            echo "OK!"
            exit 0
          fi
          echo -n .
          sleep 5
        done

check_cloudwatch: &check_cloudwatch
  platform: linux
  params:
    ACCOUNT_ROLE_ARN:
    CLUSTER_DOMAIN:
    AWS_REGION: eu-west-2
    AWS_DEFAULT_REGION: eu-west-2
    TEST_FARBACK: 180
    TEST_RETRIES: 3
    TEST_DELAY: 30
  run:
    path: /bin/bash
    args:
    - -euo
    - pipefail
    - -c
    - |
      echo "assuming aws deployer role..."
      AWS_CREDS="$(aws-assume-role $ACCOUNT_ROLE_ARN)"
      eval "${AWS_CREDS}"

      CURRENT_TIME=$(date '+%s')
      DELAY="${TEST_DELAY:-30}"
      RETRIES="${TEST_RETRIES:-3}"
      FARBACK="${TEST_FARBACK:-300}"
      LOGS_SINCE=$(($CURRENT_TIME - $FARBACK))
      LOGGROUP="$CLUSTER_DOMAIN"

      # convert from seconds based epoch to AWS supported milliseconds epoch
      CURRENT_TIME="${CURRENT_TIME}000"
      LOGS_SINCE="${LOGS_SINCE}000"

      echo "ClusterDomain: $CLUSTER_DOMAIN"
      echo "  Retry Delay: $DELAY"
      echo "      Retries: $RETRIES"
      echo "         Time: $CURRENT_TIME"
      echo "   Logs Since: $LOGS_SINCE"
      echo "    Log Group: $LOGGROUP"

      i=0
      while [ $i -lt $RETRIES ]; do
        i=$((i+1))
        echo "======================================="
        echo "      Attempt: $i"

        LASTSEENLOG=$(aws logs filter-log-events --log-group-name $LOGGROUP --start-time $LOGS_SINCE --max-items 10 | jq ".events[].timestamp" | grep -v "null" | sort -urn | head -n1)

        echo "   Logs Since: $LOGS_SINCE"
        echo "    Logs Seen: $LASTSEENLOG"
        if (( ${LASTSEENLOG} > ${LOGS_SINCE} )); then
          echo "PASS: Logs have been reached cloudwatch"
          echo "Logs received at: $LASTSEENLOG in $LOGGROUP"
          exit 0
        fi
        if (( ${i} != ${RETRIES} )); then
          echo "Retrying in ${DELAY} seconds"
          sleep ${DELAY}
        fi
      done

      echo ""
      echo "FAIL: No logs have been detected reaching cloudwatch since $LOGS_SINCE"
      exit 1

check_health_monitoring: &check_health_monitoring
  platform: linux
  params:
    ACCOUNT_ROLE_ARN:
    CLUSTER_NAME:
    AWS_REGION: eu-west-2
    AWS_DEFAULT_REGION: eu-west-2
  run:
    path: /bin/bash
    args:
    - -euo
    - pipefail
    - -c
    - |
      echo "Assuming AWS deployer role..."
      AWS_CREDS="$(aws-assume-role $ACCOUNT_ROLE_ARN)"
      eval "${AWS_CREDS}"
      echo "Fetching kubeconfig from aws..."
      aws eks update-kubeconfig --name "${CLUSTER_NAME}" --kubeconfig ./kubeconfig
      export KUBECONFIG=$(pwd)/kubeconfig
      # Check Prometheus
      echo "Port forwarding Prometheus to localhost:9090..."
      kubectl -n gsp-system port-forward service/gsp-prometheus-operator-prometheus 9090:9090 &
      sleep 5
      echo "Curling forwarded port..."
      curl --silent --show-error --max-time 5 --fail --location "http://127.0.0.1:9090" | grep "<title>Prometheus"
      echo "Success!"
      echo "Stopping port forward"
      kill $(jobs -p)
      # Check Grafana
      echo "Port forwarding Grafana to localhost:8080"
      kubectl -n gsp-system port-forward service/gsp-grafana 8080:80 &
      sleep 5
      curl --silent --show-error --max-time 5 --fail --location "http://127.0.0.1:8080" | grep "<title>Grafana</title>"
      echo "Success!"

drain_cluster_task: &drain_cluster_task
  platform: linux
  params:
    ACCOUNT_ROLE_ARN:
    CLUSTER_NAME:
    AWS_REGION: eu-west-2
    AWS_DEFAULT_REGION: eu-west-2
  run:
    path: /bin/bash
    args:
    - -eu
    - -c
    - |
      echo "assuming aws deployer role..."
      AWS_CREDS="$(aws-assume-role $ACCOUNT_ROLE_ARN)"
      eval "${AWS_CREDS}"
      echo "checking there is a cluster to drain"
      if ! (aws eks list-clusters 1>/dev/null && aws eks describe-cluster --name "${CLUSTER_NAME}" 2>/dev/null 1>/dev/null); then
        echo 'no eks cluster running: skipping drain'
        exit 0
      fi
      echo "fetching kubeconfig from aws..."
      aws eks update-kubeconfig --name "${CLUSTER_NAME}" --kubeconfig ./kubeconfig
      export KUBECONFIG=$(pwd)/kubeconfig
      echo "fetching cluster VPC ID..."
      CLUSTER_VPC_ID=$(aws eks describe-cluster --name "${CLUSTER_NAME}" | jq .cluster.resourcesVpcConfig.vpcId -r)
      echo "deleting any LoadBalancer services..."
      kubectl get svc -o json --all-namespaces | jq '.items[] | select(.spec.type == "LoadBalancer")' | kubectl delete -f - --wait
      echo "checking for any ELBs that belong to cluster..."
      ELB_ARNS_JSON=$(aws elbv2 describe-load-balancers | jq "{LoadBalancerArns: [ .LoadBalancers[] | select(.VpcId == \"${CLUSTER_VPC_ID}\") | select(.LoadBalancerName != \"${CLUSTER_NAME}-ingress\") | .LoadBalancerArn ]}" -c)
      ELB_ARNS_COUNT=$(echo $ELB_ARNS_JSON | jq '.LoadBalancerArns | length')
      echo "waiting for ${ELB_ARNS_COUNT} ELBs to terminate..."
      if [[ "${ELB_ARNS_COUNT}" != "0" ]]; then
        aws elbv2 wait load-balancers-deleted --cli-input-json "${ELB_ARNS_JSON}"
      fi
      echo "checking for ASGs that belong to this cluster..."
      CLUSTER_ASGS=$(aws autoscaling describe-auto-scaling-groups | jq -r ".AutoScalingGroups[] | select( .Tags[].Key == \"kubernetes.io/cluster/${CLUSTER_NAME}\")" | jq -r .AutoScalingGroupName)
      for ASG_NAME in $CLUSTER_ASGS; do
        echo "scaling ${ASG_NAME} to zero..."
        aws autoscaling update-auto-scaling-group --auto-scaling-group-name "${ASG_NAME}" --min-size 0 --max-size 0 --desired-capacity 0
      done
      echo "checking if any nodes are still running ..."
      for ASG_NAME in $CLUSTER_ASGS; do
        echo "checking number of instances remaining in ${ASG_NAME}..."
        INSTANCES=$(aws autoscaling describe-auto-scaling-groups --auto-scaling-group-names "${ASG_NAME}" --query "AutoScalingGroups[0].Instances[*].InstanceId" --output text)
        if [ ! -z "$INSTANCES" ]; then
          echo "waiting for following instances to terminate in ${ASG_NAME}: ${INSTANCES}..."
          aws ec2 wait instance-terminated --instance-ids $INSTANCES
        fi
      done
  inputs:
  - name: platform

resource_types:
- name: terraform
  type: registry-image
  source:
    repository: ((terraform-resource-image))
    tag: ((terraform-resource-tag))
- name: github
  type: registry-image
  source:
    repository: ((github-resource-image))
    tag: ((github-resource-tag))
- name: concourse-pipeline
  type: docker-image
  source:
    repository: concourse/concourse-pipeline-resource
    tag: "2.2.0"

resources:
- name: gsp
  type: github-release
  source:
    owner: ((platform-organization))
    repository: ((platform-repository))
    access_token: ((github-api-token))
    release: true
    pre_release: ((platform-pre-release))
    tag_filter: ((github-release-tag-prefix))v([^v].*)
- name: config
  type: github
  source:
    uri: ((config-uri))
    organization: ((config-organization))
    repository: ((config-repository))
    github_api_token: ((github-api-token))
    approvers: [chrisfarms]
    required_approval_count: 0
    branch: ((config-version))
    commit_verification_keys: ((trusted-developer-keys))
- name: users
  type: github-release
  source:
    owner: ((users-organization))
    repository: ((users-repository))
    access_token: ((github-api-token))
    release: true
- name: pipeline
  type: concourse-pipeline
  source:
    teams:
    - name: gsp
      username: gsp
      password: ((readonly_local_user_password))
- name: cluster-state
  type: terraform
  source:
    env_name: ((account-name))
    backend_type: s3
    backend_config:
      bucket: cd-gsp-private-qndvvc
      region: eu-west-2
      key: cluster-((cluster-name)).tfstate
    vars:
      account_id: ((account-id))
      account_name: ((account-name))
      cluster_name: ((cluster-name))
      cluster_domain: ((cluster-domain))
      cluster_number: ((cluster-number))
      aws_account_role_arn: ((account-role-arn))
      github_client_id: ((github-client-id))
      github_client_secret: ((github-client-secret))
      splunk_enabled: ((splunk-enabled))
      splunk_hec_url: ((splunk-hec-url))
      k8s_splunk_hec_token: ((k8s-splunk-hec-token))
      k8s_splunk_index: ((k8s-splunk-index))
      hsm_splunk_hec_token: ((hsm-splunk-hec-token))
      hsm_splunk_index: ((hsm-splunk-index))
      vpc_flow_log_splunk_hec_token: ((vpc-flow-log-splunk-hec-token))
      vpc_flow_log_splunk_index: ((vpc-flow-log-splunk-index))
      eks_version: ((eks-version))
      worker_instance_type: ((worker-instance-type))
      worker_count: ((worker-count))
      ci_worker_instance_type: ((ci-worker-instance-type))
      ci_worker_count: ((ci-worker-count))
- name: user-state
  type: terraform
  source:
    env_name: ((account-name))
    backend_type: s3
    backend_config:
      bucket: cd-gsp-private-qndvvc
      region: eu-west-2
      key: users-((cluster-name)).tfstate
    vars:
      account_id: ((account-id))
      account_name: ((account-name))
      cluster_name: ((cluster-name))
      cluster_domain: ((cluster-domain))
      cluster_number: ((cluster-number))
      aws_account_role_arn: ((account-role-arn))
      github_client_id: ((github-client-id))
      github_client_secret: ((github-client-secret))
- name: task-toolbox
  type: docker-image
  source:
    repository: ((task-toolbox-image))
    tag: ((task-toolbox-tag))

jobs:
- name: update
  serial_groups: [cluster-modification]
  plan:
  - get: task-toolbox
  - get: gsp
    trigger: ((platform-trigger))
    params:
      include_source_tarball: true
  - get: config
    trigger: ((config-trigger))
  - get: users
    trigger: ((users-trigger))
  - task: unpack-gsp-release
    image: task-toolbox
    config: *unpack_release
    params:
      CLUSTER_PUBLIC_KEY: ((ci-system-gpg-public))
  - task: generate-trusted-contributors
    image: task-toolbox
    file: platform/pipelines/tasks/generate-trusted-contributors.yaml
    params:
      ACCOUNT_NAME: ((account-name))
      CLUSTER_PUBLIC_KEY: ((ci-system-gpg-public))
  - put: pipeline
    params:
      pipelines:
      - name: ((concourse-pipeline-name))
        team: ((concourse-team))
        config_file: gsp/deployer.yaml
        vars_files:
        - gsp/deployer.defaults.yaml
        - config/((config-path))
        - trusted-contributors/github.vars.yaml
        - trusted-contributors/keys.vars.yaml

- name: deploy
  serial: true
  serial_groups: [cluster-modification]
  plan:
  - aggregate:
    - get: task-toolbox
    - get: gsp
      passed: [update]
      trigger: true
      params:
        include_source_tarball: true
    - get: config
      passed: [update]
      trigger: true
    - get: users
      passed: [update]
      trigger: true
  - task: unpack-gsp-release
    image: task-toolbox
    config: *unpack_release
    params:
      CLUSTER_PUBLIC_KEY: ((ci-system-gpg-public))
  - put: cluster-state
    params:
      env_name: ((account-name))
      terraform_source: platform/pipelines/deployer
  - task: generate-user-terraform
    image: task-toolbox
    timeout: 10m
    config: *generate_users_terraform
    params:
      ACCOUNT_NAME: ((account-name))
  - put: user-state
    params:
      env_name: ((account-name))
      terraform_source: users-terraform

- name: apply
  serial: true
  serial_groups: [cluster-modification]
  plan:
  - aggregate:
    - get: gsp
      passed: [deploy]
      trigger: true
    - get: config
      passed: [deploy]
      trigger: true
    - get: users
      passed: [deploy]
      trigger: true
    - get: task-toolbox
    - get: cluster-state
      passed: [deploy]
      trigger: true
  - aggregate:
    - task: generate-cluster-values
      image: task-toolbox
      timeout: 10m
      config: *generate_cluster_values
    - task: generate-user-values
      image: task-toolbox
      timeout: 10m
      config: *generate_user_values
      params:
        ACCOUNT_ID: ((account-id))
        CLUSTER_NAME: ((cluster-name))
        ACCOUNT_NAME: ((account-name))
    - task: generate-kube-values
      timeout: 10m
      config: *generate_kube_values
      image: task-toolbox
      params:
        ACCOUNT_ROLE_ARN: ((account-role-arn))
        CLUSTER_NAME: ((cluster-name))
        AWS_REGION: eu-west-2
        AWS_DEFAULT_REGION: eu-west-2
  - task: apply-cluster-chart
    image: task-toolbox
    timeout: 10m
    config: *apply_cluster_chart
    params:
      ACCOUNT_ROLE_ARN: ((account-role-arn))
      ACCOUNT_NAME: ((account-name))
      CLUSTER_NAME: ((cluster-name))
      AWS_REGION: eu-west-2
      AWS_DEFAULT_REGION: eu-west-2
      CHART_NAME: gsp-cluster
      DEFAULT_NAMESPACE: gsp-system
      CHART_RELEASE_NAME: gsp
      GITHUB_API_TOKEN: ((github-api-token))
      CLUSTER_PRIVATE_KEY: ((ci-system-gpg-private))
      CLUSTER_PUBLIC_KEY: ((ci-system-gpg-public))
      CONFIG_VALUES_PATH: ((config-values-path))

- name: check-canary
  plan:
  - get: task-toolbox
  - get: gsp
    passed: [apply]
    trigger: true
  - task: ping
    image: task-toolbox
    timeout: 20m
    config: *check_canary
    params:
      CLUSTER_DOMAIN: ((cluster-domain))

- name: check-logging
  plan:
  - get: task-toolbox
  - get: gsp
    passed: [apply]
    trigger: true
  - task: check-cloudwatch
    image: task-toolbox
    timeout: 10m
    config: *check_cloudwatch
    params:
      ACCOUNT_ROLE_ARN: ((account-role-arn))
      CLUSTER_DOMAIN: ((cluster-domain))

- name: check-conformance
  plan:
  - get: task-toolbox
  - get: gsp
    passed: [apply]
    trigger: true
  - task: run-conformance-tests
    image: task-toolbox
    timeout: 15m
    config: *check_conformance
    params:
      ACCOUNT_ROLE_ARN: ((account-role-arn))
      ACCOUNT_NAME: ((account-name))
      CLUSTER_NAME: ((cluster-name))
      DEFAULT_NAMESPACE: gsp-system
      AWS_REGION: eu-west-2
      AWS_DEFAULT_REGION: eu-west-2

- name: check-tools
  plan:
  - get: task-toolbox
  - get: gsp
    passed: [apply]
    trigger: true
  - task: check-monitoring-tools
    image: task-toolbox
    timeout: 10m
    config: *check_health_monitoring
    params:
      ACCOUNT_ROLE_ARN: ((account-role-arn))
      CLUSTER_NAME: ((cluster-name))

- name: destroy
  serial: true
  serial_groups: [cluster-modification]
  disable_manual_trigger: ((disable-destroy))
  plan:
  - get: task-toolbox
  - get: config
  - get: users
  - get: gsp
    params:
      include_source_tarball: true
  - task: unpack-gsp-release
    image: task-toolbox
    config: *unpack_release
    params:
      CLUSTER_PUBLIC_KEY: ((ci-system-gpg-public))
  - task: drain-cluster
    image: task-toolbox
    timeout: 30m
    config: *drain_cluster_task
    params:
      ACCOUNT_ROLE_ARN: ((account-role-arn))
      CLUSTER_NAME: ((cluster-name))
  - task: generate-user-terraform
    image: task-toolbox
    timeout: 10m
    config: *generate_users_terraform
    params:
      ACCOUNT_NAME: ((account-name))
  - put: cluster-state
    params:
      env_name: ((account-name))
      terraform_source: platform/pipelines/deployer
      action: destroy
    get_params:
      action: destroy
  - put: user-state
    params:
      env_name: ((account-name))
      terraform_source: users-terraform
      action: apply
    get_params:
      action: apply
