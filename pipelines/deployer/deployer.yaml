groups:
- name: deploy
  jobs:
  - deploy
- name: destroy
  jobs:
  - destroy

terraform_source: &terraform_source
  env_name: ((account-name))
  backend_type: s3
  backend_config: &terraform_backend_config
    bucket: cd-gsp-private-qndvvc
    region: eu-west-2
  vars:
    account_id: ((account-id))
    account_name: ((account-name))
    cluster_name: ((cluster-name))
    cluster_domain: ((cluster-name)).((account-name)).govsvc.uk
    cluster_number: ((cluster-number))
    aws_account_role_arn: ((account-role-arn))
    promotion_signing_key: ((ci-system-gpg-private))
    promotion_verification_key: ((ci-system-gpg-public))
    github_client_id: ((github-client-id))
    github_client_secret: ((github-client-secret))
    splunk_enabled: ((splunk-enabled))
    splunk_hec_token: ((splunk-hec-token))
    splunk_hec_url: ((splunk-hec-url))
    eks_version: ((eks-version))

task_image_resource: &task_image_resource
  type: docker-image
  source: {repository: "govsvc/task-toolbox", tag: "1.1.0"}

generate_cluster_values: &generate_cluster_values
  platform: linux
  image_resource: *task_image_resource
  run:
    path: /bin/bash
    args:
    - -eu
    - -c
    - |
      mkdir -p cluster-values
      echo "fetching cluster-values file from cluster-state..."
      jq -r '.values' ./cluster-state/metadata > ./cluster-values/values.yaml
      cat ./cluster-values/values.yaml
  inputs:
  - name: cluster-state
  outputs:
  - name: cluster-values

generate_user_values: &generate_user_values
  platform: linux
  image_resource: *task_image_resource
  params:
    ACCOUNT_ID: ((account-id))
    CLUSTER_NAME: ((cluster-name))
    PATH_TO_USERS: ((config-path))/users/
  run:
    path: /bin/bash
    args:
    - -eu
    - -c
    - |
      cd config
      echo "creating helm compatible values file from user data"
      cat ${PATH_TO_USERS}/*.yaml \
        | yq . \
        | jq ". + {roleARN: (\"arn:aws:iam::${ACCOUNT_ID}:role/${CLUSTER_NAME}-user-\" + .name)}" \
        | jq -s '{users: .}' \
        | yq --yaml-output .\
        > ../user-values/values.yaml
      cat ../user-values/values.yaml
  inputs:
  - name: config
  outputs:
  - name: user-values

generate_users_terraform: &generate_users_terraform
  platform: linux
  image_resource: *task_image_resource
  params:
    PATH_TO_USERS: ((config-path))/users/
  run:
    path: /bin/bash
    args:
    - -eu
    - -c
    - |
      mkdir -p users-terraform
      cd config
      echo "creating terraform for user roles from user data"
      cat ${PATH_TO_USERS}/*.yaml \
        | yq . \
        | jq '[{key: (.name | gsub("[^a-z-A-Z0-9]"; "-")), value: {source: "../platform/modules/gsp-user", user_name: .name, user_arn: .ARN, cluster_name: "${var.cluster_name}"}}] | from_entries' \
        | jq -s '{module: . | add, variable: { aws_account_role_arn: { type: "string" }, cluster_name: { type: "string" } }, provider: { aws: { region: "eu-west-2", assume_role: { role_arn: "${var.aws_account_role_arn}" } } } }' \
        > ../users-terraform/users.tf.json
      cat ../users-terraform/users.tf.json
  inputs:
  - name: config
  outputs:
  - name: users-terraform

generate_namespace_values: &generate_namespace_values
  platform: linux
  image_resource: *task_image_resource
  params:
    PATH_TO_NAMESPACES: ((config-path))/namespaces/
    CLUSTER_NAME: ((cluster-name))
  run:
    path: /bin/bash
    args:
    - -eu
    - -c
    - |
      echo "creating tmp dirs"
      mkdir -p namespace-values
      namespace_values_file="$(pwd)/namespace-values/values.yaml"
      mkdir ns-chart
      cd config/${PATH_TO_NAMESPACES}
      echo "creating helm compatible values file from namespace data"
      echo 'namespaces:' > $namespace_values_file
      for ns in gsp-* "${CLUSTER_NAME}-*"; do
        [ ! -d "${ns}" ] && continue
        echo "--> ${ns}"
        echo "- name: ${ns}" >> $namespace_values_file
        set_namespace=".metadata.namespace=\"${ns}\""
        combined_resources="$(cat ${ns}/*.yaml | yq . | jq $set_namespace | jq -s -c .)"
        echo "  resources: $combined_resources" >> $namespace_values_file
      done
      cat $namespace_values_file
  inputs:
  - name: config
  outputs:
  - name: namespace-values

apply_cluster_chart: &apply_cluster_chart
  platform: linux
  image_resource: *task_image_resource
  params:
    ACCOUNT_ROLE_ARN: ((account-role-arn))
    ACCOUNT_NAME: ((account-name))
    CLUSTER_NAME: ((cluster-name))
    AWS_REGION: eu-west-2
    AWS_DEFAULT_REGION: eu-west-2
    CHART_NAME: gsp-cluster
    DEFAULT_NAMESPACE: gsp-system
    CHART_RELEASE_NAME: gsp
  run:
    path: /bin/bash
    args:
    - -eu
    - -c
    - |
      echo "assuming aws deployer role..."
      eval $(aws-assume-role $ACCOUNT_ROLE_ARN)
      echo "fetching kubeconfig from aws..."
      aws eks update-kubeconfig \
        --name "${CLUSTER_NAME}" \
        --kubeconfig ./kubeconfig
      export KUBECONFIG=$(pwd)/kubeconfig
      echo "setting default namespace to ${DEFAULT_NAMESPACE}"
      kubectl config set-context $(kubectl config get-contexts -o name) \
        --namespace "${DEFAULT_NAMESPACE}"
      echo "rendering ${CHART_NAME} chart..."
      mkdir -p manifests
      helm template \
        --name "${CHART_RELEASE_NAME}" \
        --namespace "${DEFAULT_NAMESPACE}" \
        --values cluster-values/values.yaml \
        --values user-values/values.yaml \
        --values namespace-values/values.yaml \
        --output-dir manifests \
        "platform/charts/${CHART_NAME}"
      echo "rendering gsp-istio chart..."
      helm template \
        --name istio \
        --namespace istio-system \
        --output-dir manifests \
        platform/charts/gsp-istio
      function apply() {
        echo "applying ${1} from ${CHART_NAME} chart..."
        until kubectl apply -R -f $1; do
          echo "---> ${1} apply failed retrying in 5s..."
          sleep 5
        done
        sleep 5 # FIXME: we should do something smarter than sleep and check for success
        echo "---> ${1} applied OK!"
      }
      apply manifests/${CHART_NAME}/templates/00-aws-auth/
      apply manifests/gsp-istio
      apply manifests/${CHART_NAME}/templates/01-aws-system/
      apply manifests/
  inputs:
  - name: cluster-values
  - name: user-values
  - name: namespace-values
  - name: platform


drain_cluster_task: &drain_cluster_task
  platform: linux
  image_resource: *task_image_resource
  params:
    ACCOUNT_ROLE_ARN: ((account-role-arn))
    AWS_REGION: eu-west-2
    AWS_DEFAULT_REGION: eu-west-2
    CLUSTER_NAME: ((cluster-name))
  run:
    path: /bin/bash
    args:
    - -eu
    - -c
    - |
      echo "assuming aws deployer role..."
      eval $(aws-assume-role $ACCOUNT_ROLE_ARN)
      echo "checking there is a cluster to drain"
      if ! (aws eks list-clusters 1>/dev/null && aws eks describe-cluster --name "${CLUSTER_NAME}" 2>/dev/null 1>/dev/null); then
        echo 'no eks cluster running: skipping drain'
        exit 0
      fi
      echo "fetching kubeconfig from aws..."
      aws eks update-kubeconfig --name "${CLUSTER_NAME}" --kubeconfig ./kubeconfig
      export KUBECONFIG=$(pwd)/kubeconfig
      echo "fetching cluster VPC ID..."
      CLUSTER_VPC_ID=$(aws eks describe-cluster --name "${CLUSTER_NAME}" | jq .cluster.resourcesVpcConfig.vpcId -r)
      echo "deleting any LoadBalancer services..."
      kubectl get svc -o json --all-namespaces | jq '.items[] | select(.spec.type == "LoadBalancer")' | kubectl delete -f - --wait
      echo "checking for any ELBs that belong to cluster..."
      ELB_ARNS_JSON=$(aws elbv2 describe-load-balancers | jq "{LoadBalancerArns: [ .LoadBalancers[] | select(.VpcId == \"${CLUSTER_VPC_ID}\") | select(.LoadBalancerName != \"${CLUSTER_NAME}-ingress\") | .LoadBalancerArn ]}" -c)
      ELB_ARNS_COUNT=$(echo $ELB_ARNS_JSON | jq '.LoadBalancerArns | length')
      echo "waiting for ${ELB_ARNS_COUNT} ELBs to terminate..."
      if [[ "${ELB_ARNS_COUNT}" != "0" ]]; then
        aws elbv2 wait load-balancers-deleted --cli-input-json "${ELB_ARNS_JSON}"
      fi
      echo "checking for ASGs that belong to this cluster..."
      CLUSTER_ASGS=$(aws autoscaling describe-auto-scaling-groups | jq -r ".AutoScalingGroups[] | select( .Tags[].Key == \"kubernetes.io/cluster/${CLUSTER_NAME}\")" | jq -r .AutoScalingGroupName)
      for ASG_NAME in $CLUSTER_ASGS; do
        echo "scaling ${ASG_NAME} to zero..."
        aws autoscaling update-auto-scaling-group --auto-scaling-group-name "${ASG_NAME}" --min-size 0 --max-size 0 --desired-capacity 0
      done
      echo "checking if any nodes are still running ..."
      for ASG_NAME in $CLUSTER_ASGS; do
        echo "checking number of instances remaining in ${ASG_NAME}..."
        INSTANCES=$(aws autoscaling describe-auto-scaling-groups --auto-scaling-group-names "${ASG_NAME}" --query "AutoScalingGroups[0].Instances[*].InstanceId" --output text)
        if [ ! -z "$INSTANCES" ]; then
          echo "waiting for following instances to terminate in ${ASG_NAME}: ${INSTANCES}..."
          aws ec2 wait instance-terminated --instance-ids $INSTANCES
        fi
      done
  inputs:
  - name: platform

resource_types:
- name: terraform
  type: registry-image
  source:
    repository: "govsvc/terraform-resource"
    tag: "0.13.0-beta.2"
- name: github
  type: registry-image
  source:
    repository: "govsvc/concourse-github-resource"
    tag: "0.0.1551114195"

resources:
- name: platform
  type: git # FIXME: should be github-resource
  source:
    uri: ((platform-repository))
    organization: alphagov
    github_api_token: "((github-api-token))"
    approvers:
      - "samcrang"
      - "paroxp"
      - "chrisfarms"
      - "tlwr"
      - "blairboy362"
    required_approval_count: 2
    branch: ((platform-version))
    commit_verification_keys: ((trusted-developer-keys))
- name: config
  type: git # FIXME: should be github-resource
  source:
    uri: ((config-repository))
    organization: alphagov
    github_api_token: "((github-api-token))"
    approvers:
      - "samcrang"
      - "paroxp"
      - "chrisfarms"
      - "tlwr"
      - "blairboy362"
    required_approval_count: 2
    branch: ((config-version))
    commit_verification_keys: ((trusted-developer-keys))
- name: cluster-state
  type: terraform
  source:
    <<: *terraform_source
    backend_config:
      <<: *terraform_backend_config
      key: cluster-((cluster-name)).tfstate
- name: user-state
  type: terraform
  source:
    <<: *terraform_source
    backend_config:
      <<: *terraform_backend_config
      key: users-((cluster-name)).tfstate

jobs:
- name: deploy
  serial: true
  serial_groups: [cluster-modification]
  plan:
  - get: platform
    trigger: true
  - get: config
    trigger: true
  - put: cluster-state
    params:
      env_name: ((account-name))
      terraform_source: platform/pipelines/deployer
  - aggregate:
    - task: generate-cluster-values
      timeout: 10m
      config: *generate_cluster_values
    - task: generate-namespace-values
      timeout: 10m
      config: *generate_namespace_values
    - task: generate-user-values
      timeout: 10m
      config: *generate_user_values
    - task: generate-user-terraform
      timeout: 10m
      config: *generate_users_terraform
  - put: user-state
    params:
      env_name: ((account-name))
      terraform_source: users-terraform
  - task: apply-cluster-chart
    timeout: 10m
    config: *apply_cluster_chart
- name: destroy
  serial: true
  serial_groups: [cluster-modification]
  disable_manual_trigger: ((disable-destroy))
  plan:
  - get: config
  - get: platform
  - task: drain-cluster
    timeout: 30m
    config: *drain_cluster_task
  - task: generate-user-terraform
    timeout: 10m
    config: *generate_users_terraform
  - put: cluster-state
    params:
      env_name: ((account-name))
      terraform_source: platform/pipelines/deployer
      action: destroy
    get_params:
      action: destroy
  - put: user-state
    params:
      env_name: ((account-name))
      terraform_source: users-terraform
      action: destroy
    get_params:
      action: destroy
