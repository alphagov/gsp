groups:
- name: deploy
  jobs:
  - update
  - deploy
  - apply
  - check-canary
  - check-logging
  - check-tools
  - check-vulnerabilities
- name: destroy
  jobs:
  - destroy

unpack_release: &unpack_release
  platform: linux
  params:
    CLUSTER_PUBLIC_KEY:
    PLATFORM_RESOURCE_TYPE:
  run:
    path: /bin/bash
    args:
    - -euo
    - pipefail
    - -c
    - |
      if [ "$PLATFORM_RESOURCE_TYPE" == "git" ]; then
        # This task only necessary for the github-release resource type
        exit 0
      fi
      echo "preparing keyring to verify release..."
      echo "${CLUSTER_PUBLIC_KEY}" > key
      gpg --import key
      gpg --verify platform/source.tar.gz.asc
      echo "unpacking src tarball..."
      tar -xvf platform/source.tar.gz -C platform --strip-components=1
  inputs:
  - name: platform
  outputs:
  - name: platform

generate_cluster_values: &generate_cluster_values
  platform: linux
  run:
    path: /bin/bash
    args:
    - -euo
    - pipefail
    - -c
    - |
      mkdir -p cluster-values
      echo "fetching cluster-values file from cluster-state..."
      jq -r '.values' ./cluster-state/metadata > ./cluster-values/values.yaml
      echo "OK!"
  inputs:
  - name: cluster-state
  outputs:
  - name: cluster-values

generate_user_values: &generate_user_values
  platform: linux
  params:
    ACCOUNT_ID:
    ACCOUNT_NAME:
    CLUSTER_NAME:
  run:
    path: /bin/bash
    args:
    - -euo
    - pipefail
    - -c
    - |
      cd users
      echo "creating helm compatible values file from user data"
      yq '.[]' "${ACCOUNT_NAME}-trusted-developers.yaml" \
        | jq ". + {roleARN: (\"arn:aws:iam::${ACCOUNT_ID}:role/${CLUSTER_NAME}-user-\" + .name)}" \
        | jq -s '{users: .}' \
        | yq --yaml-output .\
        > ../user-values/values.yaml
      cat ../user-values/values.yaml
  inputs:
  - name: users
  outputs:
  - name: user-values

generate_managed_namespaces_zones: &generate_managed_namespaces_zones
  platform: linux
  params:
    CONFIG_VALUES_PATH:
  run:
    path: /bin/bash
    args:
    - -eu
    - -c
    - |
      echo "generating external dns config for managed namespaces..."
      gomplate -d config=config/${CONFIG_VALUES_PATH} -f platform/templates/managed-namespaces-zones.tf > platform/pipelines/deployer/managed-namespaces-zones.tf
  inputs:
  - name: platform
  - name: config
  outputs:
  - name: platform

generate_managed_namespaces_values: &generate_managed_namespaces_values
  platform: linux
  params:
    CONFIG_VALUES_PATH:
  run:
    path: /bin/bash
    args:
    - -eu
    - -c
    - |
      mkdir -p managed-namespaces-values
      echo "generating istio gateway values for managed namespaces..."
      gomplate -d config=config/${CONFIG_VALUES_PATH} -f platform/templates/managed-namespaces-gateways.yaml > managed-namespaces-values/gateways-values.yaml
  inputs:
  - name: platform
  - name: config
  outputs:
  - name: managed-namespaces-values

generate_users_terraform: &generate_users_terraform
  platform: linux
  params:
    ACCOUNT_NAME:
  run:
    path: /bin/bash
    args:
    - -euo
    - pipefail
    - -c
    - |
      mkdir -p users-terraform
      cd users
      echo "creating terraform for user roles from user data"
      yq '.[]' "${ACCOUNT_NAME}-trusted-developers.yaml" \
        | jq '[{key: (.name | gsub("[^a-z-A-Z0-9]"; "-")), value: {source: "../platform/modules/gsp-user", user_name: .name, user_arn: .ARN, cluster_name: "${var.cluster_name}"}}] | from_entries' \
        | jq -s '{module: . | add, variable: { aws_account_role_arn: { type: "string" }, cluster_name: { type: "string" } }, provider: { aws: { region: "eu-west-2", assume_role: { role_arn: "${var.aws_account_role_arn}" } } } }' \
        > ../users-terraform/users.tf.json
      cat ../users-terraform/users.tf.json
  inputs:
  - name: users
  outputs:
  - name: users-terraform

apply_cluster_chart: &apply_cluster_chart
  platform: linux
  params:
    ACCOUNT_ROLE_ARN:
    ACCOUNT_NAME:
    CLUSTER_NAME:
    AWS_REGION:
    AWS_DEFAULT_REGION:
    CHART_NAME:
    DEFAULT_NAMESPACE:
    CHART_RELEASE_NAME:
    GITHUB_API_TOKEN:
    CLUSTER_PUBLIC_KEY:
    CONFIG_VALUES_PATH:
    GOOGLE_OAUTH_CLIENT_ID:
    GOOGLE_OAUTH_CLIENT_SECRET:
    PLATFORM_RESOURCE_TYPE:
    PLATFORM_VERSION:
  run:
    path: /bin/bash
    args:
    - -eu
    - -c
    - |
      if [ "$PLATFORM_RESOURCE_TYPE" == "git" ]; then
        GSP_CLUSTER_CHARTS_SOURCE=platform/charts/gsp-cluster/
        GSP_ISTIO_CHARTS_SOURCE=platform/charts/gsp-istio/

        RELEASE_TAG=$PLATFORM_VERSION
      else
        # This task only necessary for the github-release resource type
        echo "preparing keyring..."
        echo "${CLUSTER_PUBLIC_KEY}" > key
        gpg --import key
        gpg --export > ~/.gnupg/pubring.gpg
        echo "verifying gsp-cluster chart signature..."
        helm verify platform/gsp-cluster-*.tgz && echo "OK!"
        echo "verifying gsp-cluster chart signature..."
        helm verify platform/gsp-istio-*.tgz && echo "OK!"

        GSP_CLUSTER_CHARTS_SOURCE=platform/gsp-cluster-*.tgz
        GSP_ISTIO_CHARTS_SOURCE=platform/gsp-istio-*.tgz

        RELEASE_TAG=$(cat platform/tag)
      fi
      echo "assuming aws deployer role..."
      AWS_CREDS="$(aws-assume-role $ACCOUNT_ROLE_ARN)"
      eval "${AWS_CREDS}"
      echo "fetching kubeconfig from aws..."
      aws eks update-kubeconfig \
        --name "${CLUSTER_NAME}" \
        --kubeconfig ./kubeconfig
      export KUBECONFIG=$(pwd)/kubeconfig
      echo "setting default namespace to ${DEFAULT_NAMESPACE}"
      kubectl config set-context $(kubectl config get-contexts -o name) \
        --namespace "${DEFAULT_NAMESPACE}"
      echo "RELEASE_TAG=${RELEASE_TAG}"
      echo "Removing EKS-provided pod security policy clusterrolebinding (if it exists)"
      kubectl delete --ignore-not-found=true clusterrolebinding eks:podsecuritypolicy:authenticated
      echo "rendering ${CHART_NAME} chart..."
      mkdir -p manifests
      mkdir -p kube-system-manifests
      helm template \
        --name "${CHART_RELEASE_NAME}" \
        --namespace "${DEFAULT_NAMESPACE}" \
        --values cluster-values/values.yaml \
        --values user-values/values.yaml \
        --values config/${CONFIG_VALUES_PATH} \
        --set "githubAPIToken=${GITHUB_API_TOKEN}" \
        --set "googleOauthClientId=${GOOGLE_OAUTH_CLIENT_ID}" \
        --set "googleOauthClientSecret=${GOOGLE_OAUTH_CLIENT_SECRET}" \
        --set "global.cluster.releaseVersion=${RELEASE_TAG}" \
        --output-dir manifests \
        $GSP_CLUSTER_CHARTS_SOURCE
      echo "rendering gsp-istio chart..."
      helm template \
        --name istio \
        --namespace istio-system \
        --output-dir manifests \
        --values config/${CONFIG_VALUES_PATH} \
        --values managed-namespaces-values/gateways-values.yaml \
        --set global.runningOnAws=true \
        --set "global.cluster.releaseVersion=${RELEASE_TAG}" \
        $GSP_ISTIO_CHARTS_SOURCE
      rm -rf manifests/gsp-istio/charts/istio-cni
      helm template \
        --name istio \
        --namespace kube-system \
        --output-dir kube-system-manifests \
        --values config/${CONFIG_VALUES_PATH} \
        --set global.runningOnAws=true \
        --set "global.cluster.releaseVersion=${RELEASE_TAG}" \
        $GSP_ISTIO_CHARTS_SOURCE
      function apply() {
        echo "applying ${1} from ${CHART_NAME} chart..."
        # TODO remove `--validate=false` from the following line
        #      when k8s >= 1.15
        until kubectl apply --validate=false -R -f $1; do
          echo "---> ${1} apply failed retrying in 5s..."
          sleep 5
        done
        sleep 5 # FIXME: we should do something smarter than sleep and check for success
        echo "---> ${1} applied OK!"
      }
      apply manifests/${CHART_NAME}/templates/00-aws-auth/
      apply manifests/gsp-istio/charts/istio-init
      apply kube-system-manifests/gsp-istio/charts/istio-cni
      apply manifests/gsp-istio/charts/istio
      apply manifests/${CHART_NAME}/templates/01-aws-system/
      apply manifests/
  inputs:
  - name: cluster-values
  - name: config
  - name: user-values
  - name: platform
  - name: managed-namespaces-values

check_conformance: &check_conformance
  platform: linux
  params:
    ACCOUNT_ROLE_ARN:
    ACCOUNT_NAME:
    CLUSTER_NAME:
    DEFAULT_NAMESPACE:
    AWS_REGION:
    AWS_DEFAULT_REGION:
  run:
    path: /bin/bash
    args:
    - -eu
    - -c
    - |
      echo "assuming aws deployer role..."
      AWS_CREDS="$(aws-assume-role $ACCOUNT_ROLE_ARN)"
      eval "${AWS_CREDS}"
      echo "fetching kubeconfig from aws..."
      aws eks update-kubeconfig \
        --name "${CLUSTER_NAME}" \
        --kubeconfig ./kubeconfig
      export KUBECONFIG=$(pwd)/kubeconfig
      echo "setting default namespace to ${DEFAULT_NAMESPACE}"
      kubectl config set-context $(kubectl config get-contexts -o name) \
        --namespace "${DEFAULT_NAMESPACE}"

      echo "beginning conformance test..."
      mkdir -p plugins/e2e/results

      function cleanup() {
        echo "cleaning up sonobuoy..."
         sonobuoy delete --wait
      }
      trap 'cleanup' INT TERM EXIT

      sonobuoy run \
        --wait \
        --sonobuoy-image "gcr.io/heptio-images/sonobuoy:v0.14.3" \
        --plugin e2e \
        --e2e-focus "Pods should be submitted and removed" \
        --kube-conformance-image "govsvc/conformance-amd64:0.0.1559644071" \
        --plugin-env e2e.ALLOWED_NOT_READY_NODES=$(kubectl get nodes --selector "! node-role.kubernetes.io/worker"  --no-headers=true | wc -l) # only wait for worker nodes

      sleep 10 # wait for results to be written
      results=$(sonobuoy retrieve)
      sonobuoy e2e ${results}
      passed=$(sonobuoy e2e ${results} --show passed | head -n1)
      failed=$(sonobuoy e2e ${results} --show failed | head -n1)

      if [[ ${passed} == "passed tests: 1" && ${failed} == "failed tests: 0" ]]; then
        echo "SUCCESS"
        exit 0
      fi

      echo "FAIL"
      exit 1


check_canary: &check_canary
  platform: linux
  params:
    CLUSTER_DOMAIN:
    CLUSTER_NAME:
  run:
    path: /bin/bash
    args:
      - -euo
      - pipefail
      - -c
      - |
        now="$(date '+%s')"
        echo "Current time: ${now}"
        echo "pinging https://canary.${CLUSTER_NAME}-main.${CLUSTER_DOMAIN}/metrics to check ingress, expecting the deployment to happen soon..."
        while true; do
          last_deploy="$(curl --silent --show-error --max-time 5 --fail https://canary.${CLUSTER_NAME}-main.${CLUSTER_DOMAIN}/metrics | awk '$1 ~ /^canary_build_timestamp/ {print $2 * 2 / 2}')"
          if [ "${last_deploy}" -ge "${now}" ]; then
            echo "OK!"
            exit 0
          fi
          echo -n .
          sleep 5
        done

check_cloudwatch: &check_cloudwatch
  platform: linux
  params:
    ACCOUNT_ROLE_ARN:
    CLUSTER_DOMAIN:
    AWS_REGION: eu-west-2
    AWS_DEFAULT_REGION: eu-west-2
    TEST_FARBACK: 180
    LOGGROUP:
  run:
    path: /bin/bash
    args:
    - -eu
    - -c
    - |
      echo "assuming aws deployer role..."
      AWS_CREDS="$(aws-assume-role $ACCOUNT_ROLE_ARN)"
      eval "${AWS_CREDS}"

      CURRENT_TIME=$(date '+%s')
      FARBACK="${TEST_FARBACK:-180}"
      LOGS_SINCE=$(($CURRENT_TIME - $FARBACK))

      if [[ -z "${LOGGROUP}" ]]; then
        echo "LOGGROUP env var not set"
        exit 1
      fi

      # convert from seconds based epoch to AWS supported milliseconds epoch
      CURRENT_TIME="${CURRENT_TIME}000"
      LOGS_SINCE="${LOGS_SINCE}000"

      echo "ClusterDomain: $CLUSTER_DOMAIN"
      echo "         Time: $CURRENT_TIME"
      echo "   Logs Since: $LOGS_SINCE"
      echo "    Log Group: $LOGGROUP"

      LOG_EVENTS=$(aws logs filter-log-events --log-group-name $LOGGROUP --start-time $LOGS_SINCE --max-items 10)
      LOG_EVENTS_COUNT=$(echo $LOG_EVENTS | jq ".events | length")
      if (( ${LOG_EVENTS_COUNT} == 0 )); then
        echo ""
        echo "FAIL: No log events collected yet"
        exit 1
      fi

      LASTSEENLOG=$(echo $LOG_EVENTS | jq ".events[].timestamp" | grep -v "null" | sort -urn | head -n1)
      echo "   Logs Since: $LOGS_SINCE"
      echo "    Logs Seen: $LASTSEENLOG"
      if (( ${LASTSEENLOG} >= ${LOGS_SINCE} )); then
        echo "PASS: Logs have been reached cloudwatch"
        echo "Logs received at: $LASTSEENLOG in $LOGGROUP"
        exit 0
      fi

      echo ""
      echo "FAIL: No logs have been detected reaching cloudwatch since $LOGS_SINCE"
      exit 1

check_health_monitoring: &check_health_monitoring
  platform: linux
  params:
    ACCOUNT_ROLE_ARN:
    CLUSTER_NAME:
    AWS_REGION: eu-west-2
    AWS_DEFAULT_REGION: eu-west-2
  run:
    path: /bin/bash
    args:
    - -euo
    - pipefail
    - -c
    - |
      echo "Assuming AWS deployer role..."
      AWS_CREDS="$(aws-assume-role $ACCOUNT_ROLE_ARN)"
      eval "${AWS_CREDS}"
      echo "Fetching kubeconfig from aws..."
      aws eks update-kubeconfig --name "${CLUSTER_NAME}" --kubeconfig ./kubeconfig
      export KUBECONFIG=$(pwd)/kubeconfig
      # Check Prometheus
      echo "Port forwarding Prometheus to localhost:9090..."
      kubectl -n gsp-system port-forward service/gsp-prometheus-operator-prometheus 9090:9090 &
      sleep 5
      echo "Curling forwarded port..."
      curl --silent --show-error --max-time 5 --fail --location "http://127.0.0.1:9090" | grep "<title>Prometheus"
      echo "Success!"
      echo "Stopping port forward"
      kill $(jobs -p)
      # Check Grafana
      echo "Port forwarding Grafana to localhost:8080"
      kubectl -n gsp-system port-forward service/gsp-grafana 8080:80 &
      sleep 5
      curl --silent --show-error --max-time 5 --fail --location "http://127.0.0.1:8080" | grep "<title>Grafana</title>"
      echo "Success!"

drain_cluster_task: &drain_cluster_task
  platform: linux
  params:
    ACCOUNT_ROLE_ARN:
    CLUSTER_NAME:
    AWS_REGION: eu-west-2
    AWS_DEFAULT_REGION: eu-west-2
  run:
    path: /bin/bash
    args:
    - -eu
    - -c
    - |
      echo "assuming aws deployer role..."
      AWS_CREDS="$(aws-assume-role $ACCOUNT_ROLE_ARN)"
      eval "${AWS_CREDS}"
      echo "---> OK"

      echo "checking there is a cluster to drain"
      if ! (aws eks list-clusters 1>/dev/null && aws eks describe-cluster --name "${CLUSTER_NAME}" 2>/dev/null 1>/dev/null); then
        echo 'no eks cluster running: skipping drain'
        exit 0
      fi
      echo "---> OK"

      echo "fetching kubeconfig from aws..."
      aws eks update-kubeconfig --name "${CLUSTER_NAME}" --kubeconfig ./kubeconfig
      export KUBECONFIG=$(pwd)/kubeconfig
      echo "---> OK"

      echo "fetching vpc id from cluster..."
      CLUSTER_VPC_ID=$(aws eks describe-cluster --name "${CLUSTER_NAME}" | jq .cluster.resourcesVpcConfig.vpcId -r)
      echo "---> OK"

      echo 'deleting concourse pipelines'
      set +e
      kubectl delete -A --all pipelines
      set -e
      echo "---> OK"

      echo 'deleting govsvc.uk custom resources'
      export RESOURCE_TYPES=$(kubectl get crd -o json | jq -r '.items[] | select (.spec.group | endswith(".govsvc.uk")) | .spec.names.singular')
      export RESOURCE_TYPES_CHARS=$(echo -n $RESOURCE_TYPES | wc -c)
      if [[ "$RESOURCE_TYPES_CHARS" -gt 0 ]]; then
        echo "deleting: $RESOURCE_TYPES"
        kubectl delete -A --all $(echo $RESOURCE_TYPES | tr ' ' ',')
      else
        echo "No resource types found"
      fi
      echo "---> OK"

      echo 'deleting PodDisruptionBudgets'
      kubectl delete -A --all poddisruptionbudget
      echo "---> OK"

      echo 'deleting deployments and statefulsets'
      kubectl delete -A --all deployment,statefulset
      echo "---> OK"

      echo 'deleting volumes and claims'
      kubectl delete -A --all pv,pvc
      echo "---> OK"

      echo "deleting any LoadBalancer services..."
      kubectl get svc -o json --all-namespaces | jq '.items[] | select(.spec.type == "LoadBalancer")' | kubectl delete -f - --wait
      echo "---> OK"

      echo "waiting for ELBs to terminate..."
      ELB_ARNS_JSON=$(aws elbv2 describe-load-balancers | jq "{LoadBalancerArns: [ .LoadBalancers[] | select(.VpcId == \"${CLUSTER_VPC_ID}\") | select(.LoadBalancerName != \"${CLUSTER_NAME}-ingress\") | .LoadBalancerArn ]}" -c)
      ELB_ARNS_COUNT=$(echo $ELB_ARNS_JSON | jq '.LoadBalancerArns | length')
      if [[ "${ELB_ARNS_COUNT}" != "0" ]]; then
        aws elbv2 wait load-balancers-deleted --cli-input-json "${ELB_ARNS_JSON}"
      fi
      echo "---> OK"

      echo "checking for ASGs that belong to this cluster..."
      CLUSTER_ASGS=$(aws autoscaling describe-auto-scaling-groups | jq -r ".AutoScalingGroups[] | select( .Tags[].Key == \"kubernetes.io/cluster/${CLUSTER_NAME}\")" | jq -r .AutoScalingGroupName)
      for ASG_NAME in $CLUSTER_ASGS; do
        echo "scaling ${ASG_NAME} to zero..."
        aws autoscaling update-auto-scaling-group --auto-scaling-group-name "${ASG_NAME}" --min-size 0 --max-size 0 --desired-capacity 0
      done
      echo "---> OK"

      echo "checking if any nodes are still running..."
      for ASG_NAME in $CLUSTER_ASGS; do
        echo "checking number of instances remaining in ${ASG_NAME}..."
        INSTANCES=$(aws autoscaling describe-auto-scaling-groups --auto-scaling-group-names "${ASG_NAME}" --query "AutoScalingGroups[0].Instances[*].InstanceId" --output text)
        if [ ! -z "$INSTANCES" ]; then
          echo "waiting for following instances to terminate in ${ASG_NAME}: ${INSTANCES}..."
          aws ec2 wait instance-terminated --instance-ids $INSTANCES
        fi
      done
      echo "---> OK"

      echo "tidying up any orphaned volumes...."
      VOLUME_IDS=$(aws ec2 describe-volumes --filters "Name=tag:kubernetes.io/cluster/${CLUSTER_NAME},Values=owned" --max-items 1000 | jq '.Volumes[].VolumeId' -r)
      for VOLUME_ID in $VOLUME_IDS; do
        aws ec2 delete-volume --volume-id "${VOLUME_ID}"
      done
      echo "---> OK"
  inputs:
  - name: platform

resource_types:
- name: terraform
  type: registry-image
  source:
    repository: ((terraform-resource-image))
    tag: ((terraform-resource-tag))
- name: github
  type: registry-image
  source:
    repository: ((github-resource-image))
    tag: ((github-resource-tag))
- name: concourse-pipeline
  type: docker-image
  source:
    repository: concourse/concourse-pipeline-resource
    tag: "2.2.0"
- name: s3-iam
  type: docker-image
  source:
    repository: governmentpaas/s3-resource
    tag: 97e441efbfb06ac7fb09786fd74c64b05f9cc907

resources:
- name: platform
  type: ((platform-resource-type))
  source:
    # Parameters for if platform-resource-type is set to github-release
    owner: ((platform-organization))
    repository: ((platform-repository))
    access_token: ((github-api-token))
    release: true
    pre_release: ((platform-pre-release))
    tag_filter: ((platform-tag-filter))

    # Parameters for if platform-resource-type is set to git
    uri: https://github.com/alphagov/gsp.git
    branch: ((platform-version))
- name: aws-node-lifecycle-hook
  type: s3-iam
  source:
    bucket: ((readonly_private_bucket_name))
    region_name: eu-west-2
    versioned_file: aws-node-lifecycle-hook.zip
- name: config
  type: ((config-resource-type))
  source:
    uri: ((config-uri))
    organization: ((config-organization))
    repository: ((config-repository))
    github_api_token: ((github-api-token))
    approvers: ((config-approvers))
    required_approval_count: ((config-approval-count))
    branch: ((config-version))
- name: users
  type: github-release
  source:
    owner: ((users-organization))
    repository: ((users-repository))
    access_token: ((github-api-token))
    release: true
- name: pipeline
  type: concourse-pipeline
  source:
    teams:
    - name: gsp
      username: gsp
      password: ((readonly_local_user_password))
- name: cluster-state
  type: terraform
  source:
    env_name: ((account-name))
    backend_type: s3
    backend_config:
      bucket: cd-gsp-private-qndvvc
      region: eu-west-2
      key: cluster-((cluster-name)).tfstate
    vars:
      account_name: ((account-name))
      cluster_name: ((cluster-name))
      cluster_domain: ((cluster-domain))
      cluster_number: ((cluster-number))
      aws_account_role_arn: ((account-role-arn))
      github_client_id: ((github-client-id))
      github_client_secret: ((github-client-secret))
      splunk_enabled: ((splunk-enabled))
      splunk_hec_url: ((splunk-hec-url))
      k8s_splunk_hec_token: ((k8s-splunk-hec-token))
      k8s_splunk_index: ((k8s-splunk-index))
      hsm_splunk_hec_token: ((hsm-splunk-hec-token))
      hsm_splunk_index: ((hsm-splunk-index))
      eks_version: ((eks-version))
      worker_eks_version: ((worker-eks-version))
      minimum_workers_per_az_count: ((minimum-workers-per-az-count))
      maximum_workers_per_az_count: ((maximum-workers-per-az-count))
      worker_on_demand_base_capacity: ((worker-on-demand-base-capacity))
      worker_on_demand_percentage_above_base: ((worker-on-demand-percentage-above-base))
      enable_nlb: ((enable-nlb))
      cls_destination_enabled: ((cls-destination-enabled))
      cls_destination_arn: ((cls-destination-arn))
- name: user-state
  type: terraform
  source:
    env_name: ((account-name))
    backend_type: s3
    backend_config:
      bucket: cd-gsp-private-qndvvc
      region: eu-west-2
      key: users-((cluster-name)).tfstate
    vars:
      cluster_name: ((cluster-name))
      aws_account_role_arn: ((account-role-arn))
- name: task-toolbox
  type: docker-image
  source:
    repository: ((task-toolbox-image))
    tag: ((task-toolbox-tag))
- name: weekdaily
  type: time
  icon: update
  source:
    start: "10:00"
    stop: "16:00"
    days: [Monday, Tuesday, Wednesday, Thursday, Friday]
    location: Europe/London
- name: dailymorning
  type: time
  icon: update
  source:
    start: "09:00"
    stop: "08:59"
    location: Europe/London

jobs:
- name: update
  serial_groups: [cluster-modification]
  plan:
  - get: task-toolbox
  - get: platform
    trigger: ((platform-trigger))
    params:
      include_source_tarball: true
  - get: config
    trigger: ((config-trigger))
  - get: users
    trigger: ((users-trigger))
  - task: unpack-gsp-release
    image: task-toolbox
    config: *unpack_release
    params:
      CLUSTER_PUBLIC_KEY: ((ci-system-gpg-public))
      PLATFORM_RESOURCE_TYPE: ((platform-resource-type))
  - task: generate-trusted-contributors
    image: task-toolbox
    file: platform/pipelines/tasks/generate-trusted-contributors.yaml
    params:
      ACCOUNT_NAME: ((account-name))
      CLUSTER_PUBLIC_KEY: ((ci-system-gpg-public))
  - put: pipeline
    params:
      pipelines:
      - name: ((concourse-pipeline-name))
        team: ((concourse-team))
        config_file: platform/pipelines/deployer/deployer.yaml
        vars_files:
        - platform/pipelines/deployer/deployer.defaults.yaml
        - config/((config-path))
        - trusted-contributors/github.vars.yaml

- name: deploy
  serial: true
  serial_groups: [cluster-modification]
  plan:
  - in_parallel:
    - get: task-toolbox
    - get: platform
      passed: [update]
      trigger: true
      params:
        include_source_tarball: true
    - get: weekdaily
      trigger: true
    - get: aws-node-lifecycle-hook
    - get: config
      passed: [update]
      trigger: true
    - get: users
      passed: [update]
      trigger: true
  - task: unpack-gsp-release
    image: task-toolbox
    config: *unpack_release
    params:
      CLUSTER_PUBLIC_KEY: ((ci-system-gpg-public))
      PLATFORM_RESOURCE_TYPE: ((platform-resource-type))
  - task: ensure-aws-node-lifecycle-hook
    image: task-toolbox
    params:
      PLATFORM_RESOURCE_TYPE: ((platform-resource-type))
    config:
      platform: linux
      run:
        path: /bin/bash
        args:
        - -euo
        - pipefail
        - -c
        - |
          if [ "$PLATFORM_RESOURCE_TYPE" == "github-release" ]; then
            echo "copying aws-node-lifecycle-hook..."
            cp platform/aws-node-lifecycle-hook.zip ./platform/modules/k8s-cluster/
          else
            echo "stealing aws-node-lifecycle-hook from latest in S3 instead of a release"
            cp aws-node-lifecycle-hook/aws-node-lifecycle-hook.zip ./platform/modules/k8s-cluster/
          fi
      inputs:
      - name: platform
      - name: aws-node-lifecycle-hook
      outputs:
      - name: platform
  - task: scale-autoscaler-down
    image: task-toolbox
    timeout: 40s
    params:
      CLUSTER_NAME: ((cluster-name))
      ACCOUNT_ROLE_ARN: ((account-role-arn))
      AWS_REGION: eu-west-2
      AWS_DEFAULT_REGION: eu-west-2
    config:
      platform: linux
      run:
        path: /bin/bash
        args:
        - -euo
        - pipefail
        - -c
        - |
          echo "assuming aws deployer role..."
          AWS_CREDS="$(aws-assume-role $ACCOUNT_ROLE_ARN)"
          eval "${AWS_CREDS}"
          echo "Looking for existing EKS cluster"
          if aws eks describe-cluster --name $CLUSTER_NAME >/dev/null; then
              echo "found it, fetching kubeconfig from aws..."
              aws eks update-kubeconfig \
                --name "${CLUSTER_NAME}" \
                --kubeconfig ./kubeconfig
              echo "done, looking for deployments"
              export KUBECONFIG=$(pwd)/kubeconfig
              if kubectl --namespace gsp-system get deployment gsp-aws-cluster-autoscaler >/dev/null; then
                  echo "found it, scaling down to 0 replicas"
                  kubectl --namespace gsp-system scale --replicas=0 deployment/gsp-aws-cluster-autoscaler
                  echo "done"
              fi
          fi
  - task: generate-terraform-var-overrides
    image: task-toolbox
    timeout: 40s
    params:
      CLUSTER_NAME: ((cluster-name))
      ACCOUNT_ROLE_ARN: ((account-role-arn))
      AWS_REGION: eu-west-2
      AWS_DEFAULT_REGION: eu-west-2
    config:
      platform: linux
      run:
        path: /bin/bash
        args:
        - -euo
        - pipefail
        - -c
        - |
          echo "assuming aws deployer role..."
          AWS_CREDS="$(aws-assume-role $ACCOUNT_ROLE_ARN)"
          eval "${AWS_CREDS}"
          echo "Looking for worker node ASGs for this cluster and make a map of their AZs to desired counts..."
          JQ_FILTER=$(echo '{desired_workers_per_az_map: [' \
            '.AutoScalingGroups[] | ' \
             "select (.Tags | from_entries .Name | startswith(\"$CLUSTER_NAME-worker-\")) | " \
            '{key: .AvailabilityZones[0], value: .DesiredCapacity}' \
          '] | from_entries}')
          aws autoscaling describe-auto-scaling-groups \
            | jq "$JQ_FILTER" \
            > desired_map.json
          echo "add worker generation timestamp var (ensures nodes are rolled periodically)..."
          DAILY_TIMESTAMP=$(jq .version.time weekdaily/input)
          jq "{worker_generation_timestamp: $DAILY_TIMESTAMP}" weekdaily/input \
            > worker_generation_timestamp.json
          echo "merging into tfvars.json..."
          mkdir -p terraform-var-overrides
          jq -s '.[0] * .[1]' \
            desired_map.json \
            worker_generation_timestamp.json \
            > terraform-var-overrides/overrides.tfvars.json
          cat terraform-var-overrides/overrides.tfvars.json
          echo "OK!"
      inputs:
      - name: weekdaily
      outputs:
      - name: terraform-var-overrides
  - task: generate-managed-namespaces-zones
    timeout: 10m
    config: *generate_managed_namespaces_zones
    image: task-toolbox
    params:
      CONFIG_VALUES_PATH: ((config-values-path))
  - put: cluster-state
    params:
      env_name: ((account-name))
      terraform_source: platform/pipelines/deployer
      var_files:
      - terraform-var-overrides/overrides.tfvars.json
  - task: generate-user-terraform
    image: task-toolbox
    timeout: 10m
    config: *generate_users_terraform
    params:
      ACCOUNT_NAME: ((account-name))
  - put: user-state
    params:
      env_name: ((account-name))
      terraform_source: users-terraform

- name: apply
  serial: true
  serial_groups: [cluster-modification]
  plan:
  - in_parallel:
    - get: platform
      passed: [deploy]
      trigger: true
    - get: config
      passed: [deploy]
      trigger: true
    - get: users
      passed: [deploy]
      trigger: true
    - get: task-toolbox
    - get: cluster-state
      passed: [deploy]
      trigger: true
  - task: unpack-gsp-release
    image: task-toolbox
    config: *unpack_release
    params:
      CLUSTER_PUBLIC_KEY: ((ci-system-gpg-public))
      PLATFORM_RESOURCE_TYPE: ((platform-resource-type))
  - in_parallel:
    - task: generate-cluster-values
      image: task-toolbox
      timeout: 10m
      config: *generate_cluster_values
    - task: generate-user-values
      image: task-toolbox
      timeout: 10m
      config: *generate_user_values
      params:
        ACCOUNT_ID: ((account-id))
        CLUSTER_NAME: ((cluster-name))
        ACCOUNT_NAME: ((account-name))
    - task: generate-managed-namespaces-values
      timeout: 10m
      config: *generate_managed_namespaces_values
      image: task-toolbox
      params:
        CONFIG_VALUES_PATH: ((config-values-path))
  - task: apply-cluster-chart
    image: task-toolbox
    timeout: 10m
    config: *apply_cluster_chart
    params:
      ACCOUNT_ROLE_ARN: ((account-role-arn))
      ACCOUNT_NAME: ((account-name))
      CLUSTER_NAME: ((cluster-name))
      AWS_REGION: eu-west-2
      AWS_DEFAULT_REGION: eu-west-2
      CHART_NAME: gsp-cluster
      DEFAULT_NAMESPACE: gsp-system
      CHART_RELEASE_NAME: gsp
      GITHUB_API_TOKEN: ((github-api-token))
      CLUSTER_PUBLIC_KEY: ((ci-system-gpg-public))
      CONFIG_VALUES_PATH: ((config-values-path))
      GOOGLE_OAUTH_CLIENT_ID: ((google-oauth-client-id))
      GOOGLE_OAUTH_CLIENT_SECRET: ((google-oauth-client-secret))
      PLATFORM_RESOURCE_TYPE: ((platform-resource-type))
      PLATFORM_VERSION: ((platform-version))
  - task: apply-cluster-config-resources
    image: task-toolbox
    timeout: 10m
    config:
      platform: linux
      inputs:
      - name: config
      params:
        ACCOUNT_ROLE_ARN: ((account-role-arn))
        AWS_DEFAULT_REGION: eu-west-2
        AWS_REGION: eu-west-2
        CLUSTER_NAME: ((cluster-name))
        DEFAULT_NAMESPACE: gsp-system
      run:
        path: /bin/bash
        args:
        - -c
        - |
          set -euo pipefail
          if [ -d "config/cluster-resources" ]
          then
            echo "cluster resources directory exists"
            echo "assuming aws deployer role..."
            AWS_CREDS="$(aws-assume-role $ACCOUNT_ROLE_ARN)"
            eval "${AWS_CREDS}"
            echo "fetching kubeconfig from aws..."
            aws eks update-kubeconfig \
              --name "${CLUSTER_NAME}" \
              --kubeconfig ./kubeconfig
            export KUBECONFIG=$(pwd)/kubeconfig
            echo "setting default namespace to ${DEFAULT_NAMESPACE}"
            kubectl config set-context $(kubectl config get-contexts -o name) \
              --namespace "${DEFAULT_NAMESPACE}"
            echo "applying cluster resources..."
            kubectl apply -R -f config/cluster-resources
          else
            echo "no cluster resources to apply"
          fi

- name: check-canary
  plan:
  - get: task-toolbox
  - get: platform
    passed: [apply]
    trigger: true
  - get: config
    passed: [apply]
    trigger: true
  - task: ping
    image: task-toolbox
    timeout: 10m
    config: *check_canary
    params:
      CLUSTER_DOMAIN: ((cluster-domain))
      CLUSTER_NAME: ((cluster-name))

- name: check-logging
  plan:
  - get: task-toolbox
  - get: platform
    passed: [apply]
    trigger: true
  - get: config
    passed: [apply]
    trigger: true
  - in_parallel:
    - task: check-container-logs
      attempts: 3
      image: task-toolbox
      timeout: 10m
      config: *check_cloudwatch
      params:
        ACCOUNT_ROLE_ARN: ((account-role-arn))
        CLUSTER_DOMAIN: ((cluster-domain))
        LOGGROUP: /aws/containerinsights/((cluster-name))/application
    - task: check-host-logs
      attempts: 3
      image: task-toolbox
      timeout: 10m
      config: *check_cloudwatch
      params:
        ACCOUNT_ROLE_ARN: ((account-role-arn))
        CLUSTER_DOMAIN: ((cluster-domain))
        LOGGROUP: /aws/containerinsights/((cluster-name))/host
    - task: check-dataplane-logs
      attempts: 3
      image: task-toolbox
      timeout: 10m
      config: *check_cloudwatch
      params:
        ACCOUNT_ROLE_ARN: ((account-role-arn))
        CLUSTER_DOMAIN: ((cluster-domain))
        LOGGROUP: /aws/containerinsights/((cluster-name))/dataplane
    - task: check-controlplane-logs
      attempts: 3
      image: task-toolbox
      timeout: 10m
      config: *check_cloudwatch
      params:
        ACCOUNT_ROLE_ARN: ((account-role-arn))
        CLUSTER_DOMAIN: ((cluster-domain))
        LOGGROUP: /aws/eks/((cluster-name))/cluster

- name: check-tools
  plan:
  - get: task-toolbox
  - get: platform
    passed: [apply]
    trigger: true
  - get: config
    passed: [apply]
    trigger: true
  - task: check-monitoring-tools
    attempts: 5
    image: task-toolbox
    timeout: 10m
    config: *check_health_monitoring
    params:
      ACCOUNT_ROLE_ARN: ((account-role-arn))
      CLUSTER_NAME: ((cluster-name))

- name: check-vulnerabilities
  plan:
  - get: dailymorning
    trigger: true
  - get: task-toolbox
  - get: platform
    passed: [apply]
    trigger: true
  - get: config
    passed: [apply]
    trigger: true
  - task: check-vulnerabilities
    image: task-toolbox
    timeout: 600s
    params:
      CLUSTER_NAME: ((cluster-name))
      ACCOUNT_ROLE_ARN: ((account-role-arn))
      AWS_REGION: eu-west-2
      AWS_DEFAULT_REGION: eu-west-2
    privileged: true
    config:
      platform: linux
      run:
        path: /bin/bash
        args:
        - -euo
        - pipefail
        - -c
        - |
          echo "assuming aws deployer role..."
          AWS_CREDS="$(aws-assume-role $ACCOUNT_ROLE_ARN)"
          eval "${AWS_CREDS}"
          echo "updating EKS config"
          aws eks update-kubeconfig \
              --name "${CLUSTER_NAME}" \
              --kubeconfig ./kubeconfig
          echo "done, looking for CVEs"
          export KUBECONFIG=$(pwd)/kubeconfig
          /usr/bin/dockerd &
          python3 /usr/local/bin/findCVEs.py

- name: destroy
  serial: true
  serial_groups: [cluster-modification]
  disable_manual_trigger: ((disable-destroy))
  plan:
  - get: task-toolbox
  - get: config
  - get: users
  - get: platform
    params:
      include_source_tarball: true
  - task: unpack-gsp-release
    image: task-toolbox
    config: *unpack_release
    params:
      CLUSTER_PUBLIC_KEY: ((ci-system-gpg-public))
      PLATFORM_RESOURCE_TYPE: ((platform-resource-type))
  - task: drain-cluster
    attempts: 3
    image: task-toolbox
    config: *drain_cluster_task
    params:
      ACCOUNT_ROLE_ARN: ((account-role-arn))
      CLUSTER_NAME: ((cluster-name))
  - task: generate-user-terraform
    image: task-toolbox
    timeout: 10m
    config: *generate_users_terraform
    params:
      ACCOUNT_NAME: ((account-name))
  - task: remove-prevent-destroy
    image: task-toolbox
    timeout: 10s
    config:
      platform: linux
      run:
        path: /bin/bash
        args:
        - -euo
        - pipefail
        - -c
        - |
          sed -i'' -e '/prevent_destroy = true/d' platform/modules/k8s-cluster/main.tf platform/modules/gsp-subnet/public.tf
      inputs:
      - name: platform
      outputs:
      - name: platform
  - task: hack-aws-node-lifecycle-hook-zip
    image: task-toolbox
    timeout: 10s
    config:
      platform: linux
      run:
        path: /bin/bash
        args:
        - -euo
        - pipefail
        - -c
        - |
          # This is here in case we are running off the git repo instead of a release, we won't have the built ZIP but Terraform expects it to generate the hash, even on destroy
          touch platform/modules/k8s-cluster/aws-node-lifecycle-hook.zip
      inputs:
      - name: platform
      outputs:
      - name: platform
  - task: generate-managed-namespaces-zones
    timeout: 10m
    config: *generate_managed_namespaces_zones
    image: task-toolbox
    params:
      CONFIG_VALUES_PATH: ((config-values-path))
  - put: cluster-state
    params:
      env_name: ((account-name))
      terraform_source: platform/pipelines/deployer
      action: destroy
    get_params:
      action: destroy
  - put: user-state
    params:
      env_name: ((account-name))
      terraform_source: users-terraform
      action: destroy
    get_params:
      action: destroy
